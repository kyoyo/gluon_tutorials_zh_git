{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 延迟执行\n",
    "\n",
    "MXNet使用**延迟执行**来提升系统性能。绝大情况下我们不用知道它的存在，因为它不会对正常使用带来影响。但理解它的工作原理有助于开发更高效的程序。\n",
    "\n",
    "延迟执行是指命令可以等到之后它的结果真正的需要的时候再执行。我们先来看一个例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "a = 1 + 1\n",
    "# some other things\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一句对`a`赋值，再执行一些其指令后打印`a`的结果。因为这里我们可能很久以后才用`a`的值，所以我们可以把它的执行延迟到后面。这样的主要好处是在执行之前系统可以看到后面指令，从而有更多机会来对程序进行优化。例如如果`a`在被使用前被重新赋值了，那么我们可以不需要真正执行第一条语句。\n",
    "\n",
    "在MXNet里，我们把用户打交道的部分叫做前端。例如这个教程里我们一直在使用Python前端写代码。除了Python外，MXNet还支持其他例如Scala，R，C++的前端。不管使用什么前端，MXNet的程序执行主要都在C++后端。前端只是把程序传给后端。后端有自己的线程来不断的收集任务，构造计算图，优化，并执行。本章我们介绍后端优化之一：延迟执行。\n",
    "\n",
    "考虑下图的样例，我们在前端调用四条语句，它们被后端的线程分析依赖并构建成计算图。\n",
    "\n",
    "![](../img/frontend-backend.svg)\n",
    "\n",
    "在延迟执行中，前端执行前三个语句的时候，它仅仅是把任务放进后端的队列里就返回了。当在需要打印结果时，前端会等待后端线程把`c`的结果计算完。\n",
    "\n",
    "这个设计的一个好处是前端，就是Python线程，不需要做实际计算工作，从而不管Python的性能如何，它对整个程序的影响会很小。只需要C++后端足够高效，那么不管前端语言性能如何，都可以提供一致的性能。\n",
    "\n",
    "下面的例子通过计时来展示了延后执行的效果。可以看到，当`y=...`返回的时候并没有等待它真的被计算完。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workloads are queued:\t0.000665 sec\n",
      "\n",
      "[[ 501.98400879  507.21395874  485.15576172 ...,  491.92022705\n",
      "   497.25241089  483.55505371]\n",
      " [ 491.52926636  503.83093262  480.88876343 ...,  495.77688599\n",
      "   500.18539429  492.50854492]\n",
      " [ 518.2310791   521.22814941  499.03594971 ...,  510.80603027\n",
      "   508.33139038  505.09320068]\n",
      " ..., \n",
      " [ 503.28393555  506.78427124  490.05950928 ...,  501.06356812\n",
      "   500.14096069  493.59692383]\n",
      " [ 512.7734375   513.4161377   497.60931396 ...,  499.51367188\n",
      "   502.92895508  494.03161621]\n",
      " [ 514.21496582  517.6932373   496.9541626  ...,  508.26495361\n",
      "   503.22174072  498.45629883]]\n",
      "<NDArray 2000x2000 @cpu(0)>\n",
      "workloads are finished:\t0.190493 sec\n"
     ]
    }
   ],
   "source": [
    "from mxnet import nd\n",
    "from time import time\n",
    "\n",
    "start = time()\n",
    "x = nd.random_uniform(shape=(2000,2000))\n",
    "y = nd.dot(x, x)\n",
    "print('workloads are queued:\\t%f sec' % (time() - start))\n",
    "print(y)\n",
    "print('workloads are finished:\\t%f sec' % (time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "延迟执行大部分情况是对用户透明的。因为除非我们需要打印或者保存结果外，我们基本不需要关心目前是不是结果在内存里面已经计算好了。\n",
    "\n",
    "事实上，只要数据是保存在NDArray里，而且使用MXNet提供的运算子，后端将默认使用延迟执行来获取最大的性能。\n",
    "\n",
    "## 立即获取结果\n",
    "\n",
    "除了前面介绍的`print`外，我们还有别的方法可以让前端线程等待直到结果完成。我们可以使用`nd.NDArray.wait_to_read()`等待直到特定结果完成，或者`nd.waitall()`等待所有前面结果完成。后者是测试性能常用方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12470579147338867"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "y = nd.dot(x, x)\n",
    "y.wait_to_read()\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24616456031799316"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "y = nd.dot(x, x)\n",
    "z = nd.dot(x, x)\n",
    "nd.waitall()\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任何方法将内容从NDArray搬运到其他不支持延迟执行的数据结构里都会触发等待，例如`asnumpy()`, `asscalar()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13100266456604004"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "y = nd.dot(x, x)\n",
    "y.asnumpy()\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12163496017456055"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "y = nd.dot(x, x)\n",
    "y.norm().asscalar()\n",
    "time() - start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 延迟执行带来的便利\n",
    "\n",
    "下面例子中，我们不断的对`y`进行赋值。如果每次我们需要等到`y`的值，那么我们必须要要计算它。而在延迟执行里，系统有可能省略掉一些执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No lazy evaluation: 0.775486 sec\n",
      "With evaluation: 0.179942 sec\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "for i in range(1000):\n",
    "    y = x + 1\n",
    "    y.wait_to_read()\n",
    "\n",
    "print('No lazy evaluation: %f sec' % (time()-start))\n",
    "\n",
    "start = time()\n",
    "for i in range(1000):\n",
    "    y = x + 1\n",
    "nd.waitall()\n",
    "print('With evaluation: %f sec' % (time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 延迟执行带来的影响\n",
    "\n",
    "在延迟执行里，只要最终结果是一致的，系统可能使用跟代码不一样的顺序来执行，例如假设我们写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 1\n",
    "b = 2\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一句和第二句之间没有依赖，所以把`b=2`提前到`a=1`前执行也是可以的。但这样可能会导致内存使用的变化。\n",
    "\n",
    "下面我们列举几个在训练和预测中常见的现象。一般每个批量我们都会评测一下，例如计算损失或者精度，其中会用到`asscalar`或者`asnumpy`。这样我们会每次仅仅将一个批量的任务放进后端系统执行。但如果我们去掉这些同步函数，会导致我们将大量的批量任务同时放进系统，从而可能导致系统占用过多资源。\n",
    "\n",
    "为了演示这种情况，我们定义一个数据获取函数，它会打印什么数据是什么时候被请求的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    start = time()\n",
    "    batch_size = 1024\n",
    "    for i in range(60):\n",
    "        if i % 10 == 0:\n",
    "            print('batch %d, time %f sec' %(i, time()-start))\n",
    "        x = nd.zeros((batch_size, 1024))\n",
    "        y = nd.zeros((batch_size,))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用两层网络和和L2损失函数作为样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(\n",
    "        nn.Dense(1024),\n",
    "        nn.Activation('relu'),\n",
    "        nn.Dense(1024),\n",
    "    )\n",
    "net.initialize()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {})\n",
    "loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们定义辅助函数来监测内存的使用（只能在Linux运行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def get_mem():\n",
    "    \"\"\"get memory usage in MB\"\"\"\n",
    "    res = subprocess.check_output(['ps', 'u', '-p', str(os.getpid())])\n",
    "    return int(str(res).split()[15])/1e3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以做测试了。我们先试运行一次让系统把`net`的参数初始化（回忆[延后初始化](../chapter_gluon-basics/parameters.md)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, time 0.000002 sec\n"
     ]
    }
   ],
   "source": [
    "for x, y in get_data():\n",
    "    break\n",
    "loss(y, net(x)).wait_to_read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们用`net`来做预测，正常情况下对每个批量的结果我们把它复制出NDArray，例如打印或者保存在磁盘上。这里我们简单使用`wait_to_read`来模拟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, time 0.000003 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 10, time 0.670714 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 20, time 1.331492 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30, time 1.989218 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 40, time 2.616631 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 50, time 3.310513 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increased memory 3.452000 MB\n"
     ]
    }
   ],
   "source": [
    "mem = get_mem()\n",
    "\n",
    "for x, y in get_data():\n",
    "    loss(y, net(x)).wait_to_read()\n",
    "nd.waitall()\n",
    "\n",
    "print('Increased memory %f MB' % (get_mem() - mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们不使用`wait_to_read()`， 那么前端会将所有批量的计算一次性的添加进后端。可以看到每个批量的数据都会在很短的时间内生成，同时在接下来的数秒钟内，我们看到了内存的增长（包括了在内存中保存所有`x`和`y`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, time 0.000002 sec\n",
      "batch 10, time 0.006296 sec\n",
      "batch 20, time 0.010973 sec\n",
      "batch 30, time 0.015612 sec\n",
      "batch 40, time 0.020281 sec\n",
      "batch 50, time 0.024859 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increased memory 137.596000 MB\n"
     ]
    }
   ],
   "source": [
    "mem = get_mem()\n",
    "\n",
    "for x, y in get_data():\n",
    "    loss(y, net(x))\n",
    "\n",
    "nd.waitall()\n",
    "print('Increased memory %f MB' % (get_mem() - mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样对于训练，如果我们每次计算损失，那么就加入了同步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, time 0.000002 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 10, time 1.710598 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 20, time 3.694613 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 30, time 5.586327 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 40, time 7.433568 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 50, time 9.309987 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increased memory -122.308000 MB\n"
     ]
    }
   ],
   "source": [
    "from mxnet import autograd\n",
    "\n",
    "mem = get_mem()\n",
    "\n",
    "total_loss = 0\n",
    "for x, y in get_data():\n",
    "    with autograd.record():\n",
    "        L = loss(y, net(x))\n",
    "    total_loss += L.sum().asscalar()\n",
    "    L.backward()\n",
    "    trainer.step(x.shape[0])\n",
    "\n",
    "nd.waitall()\n",
    "print('Increased memory %f MB' % (get_mem() - mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但如果不去掉同步，同样会首先把数据全部生成好，导致占用大量内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0, time 0.000003 sec\n",
      "batch 10, time 0.013643 sec\n",
      "batch 20, time 0.024703 sec\n",
      "batch 30, time 0.035669 sec\n",
      "batch 40, time 0.046614 sec\n",
      "batch 50, time 0.057505 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Increased memory 235.580000 MB\n"
     ]
    }
   ],
   "source": [
    "from mxnet import autograd\n",
    "\n",
    "mem = get_mem()\n",
    "\n",
    "total_loss = 0\n",
    "for x, y in get_data():\n",
    "    with autograd.record():\n",
    "        L = loss(y, net(x))\n",
    "    L.backward()\n",
    "    trainer.step(x.shape[0])\n",
    "\n",
    "nd.waitall()\n",
    "print('Increased memory %f MB' % (get_mem() - mem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "延后执行使得系统有更多空间来做性能优化。但我们推荐每个批量里至少有一个同步函数，例如对损失函数进行评估，来避免将过多任务同时丢进后端系统。\n",
    "\n",
    "## 练习\n",
    "\n",
    "为什么同步版本的训练中，我们看到了内存使用的大量下降？\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/1881)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}