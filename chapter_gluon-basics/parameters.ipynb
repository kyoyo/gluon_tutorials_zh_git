{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 初始化模型参数\n",
    "\n",
    "我们仍然用MLP这个例子来详细解释如何初始化模型参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "46"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "\n",
    "def get_net():\n",
    "    net = nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(nn.Dense(4, activation=\"relu\"))\n",
    "        net.add(nn.Dense(2))\n",
    "    return net\n",
    "\n",
    "x = nd.random.uniform(shape=(3,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道如果不`initialize()`直接跑forward，那么系统会抱怨说参数没有初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "33"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter sequential0_dense0_weight has not been initialized. Note that you should initialize parameters and create Trainer with Block.collect_params() instead of Block.params because the later does not include Parameters of nested child Blocks"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "try:\n",
    "    net = get_net()\n",
    "    net(x)\n",
    "except RuntimeError as err:\n",
    "    sys.stderr.write(str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正确的打开方式是这样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "34"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[[  5.69327455e-03   5.31650949e-05]\n",
       " [  3.97902681e-03   7.88165082e-04]\n",
       " [  3.20330053e-03   1.50499865e-03]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 访问模型参数\n",
    "\n",
    "之前我们提到过可以通过`weight`和`bias`访问`Dense`的参数，他们是`Parameter`这个类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "35"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:  sequential0_dense0 \n",
      "weight:  Parameter sequential0_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>) \n",
      "bias:  Parameter sequential0_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n"
     ]
    }
   ],
   "source": [
    "w = net[0].weight\n",
    "b = net[0].bias\n",
    "print('name: ', net[0].name, '\\nweight: ', w, '\\nbias: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们可以通过`data`来访问参数，`grad`来访问对应的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "43"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight: \n",
      "[[ 0.01847461 -0.03004881 -0.02461551 -0.01465906 -0.05932271]\n",
      " [-0.0595007   0.0434817   0.04195441  0.05774786  0.00482907]\n",
      " [ 0.04922146  0.0243923  -0.06268584  0.04367422  0.03679534]\n",
      " [-0.06364554  0.03010933  0.05611894 -0.02152951  0.03825361]]\n",
      "<NDArray 4x5 @cpu(0)>\n",
      "weight gradient \n",
      "[[ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n",
      "<NDArray 4x5 @cpu(0)>\n",
      "bias: \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "bias gradient \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "print('weight:', w.data())\n",
    "print('weight gradient', w.grad())\n",
    "print('bias:', b.data())\n",
    "print('bias gradient', b.grad())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们也可以通过`collect_params`来访问Block里面所有的参数（这个会包括所有的子Block）。它会返回一个名字到对应Parameter的dict。既可以用正常`[]`来访问参数，也可以用`get()`，它不需要填写名字的前缀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential0_ (\n",
      "  Parameter sequential0_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential0_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential0_dense1_weight (shape=(2, 4), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential0_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
      ")\n",
      "\n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n",
      "\n",
      "[[ 0.01847461 -0.03004881 -0.02461551 -0.01465906 -0.05932271]\n",
      " [-0.0595007   0.0434817   0.04195441  0.05774786  0.00482907]\n",
      " [ 0.04922146  0.0243923  -0.06268584  0.04367422  0.03679534]\n",
      " [-0.06364554  0.03010933  0.05611894 -0.02152951  0.03825361]]\n",
      "<NDArray 4x5 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "params = net.collect_params()\n",
    "print(params)\n",
    "print(params['sequential0_dense0_bias'].data())\n",
    "print(params.get('dense0_weight').data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用不同的初始函数来初始化\n",
    "\n",
    "我们一直在使用默认的`initialize`来初始化权重（除了指定GPU `ctx`外）。它会把所有权重初始化成在`[-0.07, 0.07]`之间均匀分布的随机数。我们可以使用别的初始化方法。例如使用均值为0，方差为0.02的正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 0.01925707  0.00956226  0.0084907   0.03457717 -0.00640247]\n",
      " [ 0.03445733  0.04135119 -0.01956459  0.0094062  -0.03433602]\n",
      " [ 0.00600536  0.00486903  0.00139216  0.03815848  0.02499754]\n",
      " [-0.00589869 -0.03443903 -0.0393823  -0.0066522  -0.01360089]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "from mxnet import init\n",
    "params.initialize(init=init.Normal(sigma=0.02), force_reinit=True)\n",
    "print(net[0].weight.data(), net[0].bias.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看得更加清楚点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]\n",
      " [ 1.  1.  1.  1.  1.]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "params.initialize(init=init.One(), force_reinit=True)\n",
    "print(net[0].weight.data(), net[0].bias.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "更多的方法参见[init的API](https://mxnet.incubator.apache.org/api/python/optimization.html#the-mxnet-initializer-package). 下面我们自定义一个初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight (4, 5)\n",
      "init weight (2, 4)\n",
      "\n",
      "[[ 6.92873859  5.43979263  9.19637489  6.07963848  7.22268105]\n",
      " [ 5.52318239  9.39675808  8.25591183  7.59210968  5.14339209]\n",
      " [ 7.52368546  8.17871094  6.14734745  5.996562    6.95708847]\n",
      " [ 8.30252838  6.86959934  9.07629967  9.43707752  7.62580872]]\n",
      "<NDArray 4x5 @cpu(0)> \n",
      "[ 0.  0.  0.  0.]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "class MyInit(init.Initializer):\n",
    "    def __init__(self):\n",
    "        super(MyInit, self).__init__()\n",
    "        self._verbose = True\n",
    "    def _init_weight(self, _, arr):\n",
    "        # 初始化权重，使用out=arr后我们不需指定形状\n",
    "        print('init weight', arr.shape)\n",
    "        nd.random.uniform(low=5, high=10, out=arr)\n",
    "    def _init_bias(self, _, arr):\n",
    "        print('init bias', arr.shape)\n",
    "        # 初始化偏移\n",
    "        arr[:] = 2\n",
    "\n",
    "# FIXME: init_bias doesn't work\n",
    "params.initialize(init=MyInit(), force_reinit=True)\n",
    "print(net[0].weight.data(), net[0].bias.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 延后的初始化\n",
    "\n",
    "我们之前提到过Gluon的一个便利的地方是模型定义的时候不需要指定输入的大小，在之后做forward的时候会自动推测参数的大小。我们具体来看这是怎么工作的。\n",
    "\n",
    "新创建一个网络，然后打印参数。你会发现两个全连接层的权重的形状里都有0。 这是因为在不知道输入数据的情况下，我们无法判断它们的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential1_ (\n",
      "  Parameter sequential1_dense0_weight (shape=(4, 0), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense1_weight (shape=(2, 0), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = get_net()\n",
    "print(net.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize(init=MyInit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你会看到我们并没有看到MyInit打印的东西，这是因为我们仍然不知道形状。真正的初始化发生在我们看到数据时。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight (4, 5)\n",
      "init weight (2, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "[[ 702.39978027  624.83215332]\n",
       " [ 666.17633057  593.84613037]\n",
       " [ 526.71362305  468.61410522]]\n",
       "<NDArray 3x2 @cpu(0)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这时候我们看到shape里面的0被填上正确的值了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequential1_ (\n",
      "  Parameter sequential1_dense0_weight (shape=(4, 5), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense0_bias (shape=(4,), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense1_weight (shape=(2, 4), dtype=<class 'numpy.float32'>)\n",
      "  Parameter sequential1_dense1_bias (shape=(2,), dtype=<class 'numpy.float32'>)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 避免延后初始化\n",
    "\n",
    "有时候我们不想要延后初始化，这时候可以在创建网络的时候指定输入大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight (4, 5)\n",
      "init weight (2, 4)\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(4, in_units=5, activation=\"relu\"))\n",
    "    net.add(nn.Dense(2, in_units=4))\n",
    "\n",
    "net.initialize(MyInit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共享模型参数\n",
    "\n",
    "有时候我们想在层之间共享同一份参数，我们可以通过Block的`params`输出参数来手动指定参数，而不是让系统自动生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()\n",
    "with net.name_scope():\n",
    "    net.add(nn.Dense(4, in_units=4, activation=\"relu\"))\n",
    "    net.add(nn.Dense(4, in_units=4, activation=\"relu\", params=net[-1].params))\n",
    "    net.add(nn.Dense(2, in_units=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化然后打印"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init weight (4, 4)\n",
      "init weight (2, 4)\n",
      "\n",
      "[[ 7.72266722  5.12258196  7.70183563  9.58588409]\n",
      " [ 5.34961605  5.22156763  6.64358377  5.61120892]\n",
      " [ 6.21387911  8.66970825  5.88276386  5.25478649]\n",
      " [ 6.20189953  9.93853378  8.26059151  8.12326527]]\n",
      "<NDArray 4x4 @cpu(0)>\n",
      "\n",
      "[[ 7.72266722  5.12258196  7.70183563  9.58588409]\n",
      " [ 5.34961605  5.22156763  6.64358377  5.61120892]\n",
      " [ 6.21387911  8.66970825  5.88276386  5.25478649]\n",
      " [ 6.20189953  9.93853378  8.26059151  8.12326527]]\n",
      "<NDArray 4x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "net.initialize(MyInit())\n",
    "print(net[0].weight.data())\n",
    "print(net[1].weight.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "\n",
    "我们可以很灵活地访问和修改模型参数。\n",
    "\n",
    "## 练习\n",
    "\n",
    "1. 研究下`net.collect_params()`返回的是什么？`net.params`呢？\n",
    "1. 如何对每个层使用不同的初始化函数\n",
    "1. 如果两个层共用一个参数，那么求梯度的时候会发生什么？\n",
    "\n",
    "**吐槽和讨论欢迎点**[这里](https://discuss.gluon.ai/t/topic/987)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}